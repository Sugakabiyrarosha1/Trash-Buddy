{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóëÔ∏è Trash-Buddy Model Interpretability & Explainability\n",
    "\n",
    "## Overview\n",
    "This notebook handles the seventh step of the Trash-Buddy pipeline: **Model Interpretability & Explainability**. We analyze and visualize how the model makes predictions by:\n",
    "\n",
    "- Grad-CAM visualizations (Class Activation Maps)\n",
    "- Feature importance analysis\n",
    "- SHAP values for model explanations\n",
    "- Attention maps\n",
    "- Misclassification analysis with explanations\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Prerequisites\n",
    "\n",
    "From previous steps, we have:\n",
    "- **Trained model** saved as checkpoint (from Step 3)\n",
    "- **Test dataset** for analysis (from Step 2)\n",
    "- **Label classes** and encoders (from Step 2)\n",
    "- **Evaluation results** (from Step 4)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "1. Load trained model and sample images\n",
    "2. Generate Grad-CAM visualizations\n",
    "3. Analyze feature importance\n",
    "4. Compute SHAP values for explanations\n",
    "5. Visualize attention maps\n",
    "6. Analyze misclassifications with explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\" Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Load Model and Data\n",
    "\n",
    "Load the trained model and prepare sample images for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "models_dir = Path('models')\n",
    "processed_data_dir = Path('processed_data')\n",
    "\n",
    "# Load model checkpoint\n",
    "model_checkpoint_path = list(models_dir.glob('best_model_*.pth'))[0]\n",
    "checkpoint = torch.load(model_checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "# Get model configuration\n",
    "model_config = checkpoint.get('config', {})\n",
    "MODEL_NAME = model_config.get('MODEL_NAME','resnet50')\n",
    "IMAGE_SIZE = model_config.get('IMAGE_SIZE', 224)\n",
    "IMAGENET_MEAN = model_config.get('IMAGENET_MEAN', [0.485, 0.456, 0.406])\n",
    "IMAGENET_STD = model_config.get('IMAGENET_STD', [0.229, 0.224, 0.225])\n",
    "\n",
    "# Load label classes\n",
    "label_classes = np.load(processed_data_dir /'label_classes.npy', allow_pickle=True)\n",
    "NUM_CLASSES = len(label_classes)\n",
    "\n",
    "# Create label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = label_classes\n",
    "\n",
    "# Load test split\n",
    "df_test = pd.read_csv(processed_data_dir /'test_split.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA LOADING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n Model loaded: {MODEL_NAME}\")\n",
    "print(f\" Trained for {checkpoint['epoch']} epochs\")\n",
    "print(f\" Validation Accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "print(f\"\\n Test set loaded: {len(df_test):,} images\")\n",
    "print(f\"\\n Label classes loaded: {NUM_CLASSES} classes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Model Architecture Recreation\n",
    "\n",
    "Recreate the model architecture and load weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model architecture\n",
    "def create_model(model_name='resnet50', num_classes=18, pretrained=False):\n",
    "\"\"\"Create model architecture\"\"\"\n",
    " try:\n",
    " if model_name =='resnet50':\n",
    " weights = models.ResNet50_Weights.DEFAULT if pretrained else None\n",
    " model = models.resnet50(weights=weights)\n",
    " num_features = model.fc.in_features\n",
    " model.fc = nn.Linear(num_features, num_classes)\n",
    " elif model_name =='efficientnet_b0':\n",
    " weights = models.EfficientNet_B0_Weights.DEFAULT if pretrained else None\n",
    " model = models.efficientnet_b0(weights=weights)\n",
    " num_features = model.classifier[1].in_features\n",
    " model.classifier[1] = nn.Linear(num_features, num_classes)\n",
    " elif model_name =='mobilenet_v2':\n",
    " weights = models.MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
    " model = models.mobilenet_v2(weights=weights)\n",
    " num_features = model.classifier[1].in_features\n",
    " model.classifier[1] = nn.Linear(num_features, num_classes)\n",
    " else:\n",
    " raise ValueError(f\"Unknown model: {model_name}\")\n",
    " except AttributeError:\n",
    " # Fallback for older torchvision versions\n",
    " if model_name =='resnet50':\n",
    " model = models.resnet50(pretrained=pretrained)\n",
    " model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    " elif model_name =='efficientnet_b0':\n",
    " model = models.efficientnet_b0(pretrained=pretrained)\n",
    " model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    " elif model_name =='mobilenet_v2':\n",
    " model = models.mobilenet_v2(pretrained=pretrained)\n",
    " model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    " return model\n",
    "\n",
    "# Create and load model\n",
    "model = create_model(MODEL_NAME, NUM_CLASSES, pretrained=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\" Model architecture recreated and weights loaded\")\n",
    "print(f\" Model: {MODEL_NAME}\")\n",
    "print(f\" Number of classes: {NUM_CLASSES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Grad-CAM Visualizations\n",
    "\n",
    "Generate Grad-CAM (Gradient-weighted Class Activation Mapping) visualizations to see which parts of the image the model focuses on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM implementation\n",
    "class GradCAM:\n",
    " def __init__(self, model, target_layer):\n",
    " self.model = model\n",
    " self.target_layer = target_layer\n",
    " self.gradients = None\n",
    " self.activations = None\n",
    " \n",
    " # Register hooks\n",
    " self.target_layer.register_forward_hook(self.save_activation)\n",
    " self.target_layer.register_full_backward_hook(self.save_gradient)\n",
    " \n",
    " def save_activation(self, module, input, output):\n",
    " self.activations = output\n",
    " \n",
    " def save_gradient(self, module, grad_input, grad_output):\n",
    " self.gradients = grad_output[0]\n",
    " \n",
    " def generate_cam(self, input_image, class_idx=None):\n",
    " # Forward pass\n",
    " output = self.model(input_image)\n",
    " \n",
    " if class_idx is None:\n",
    " class_idx = output.argmax(dim=1)\n",
    " \n",
    " # Backward pass\n",
    " self.model.zero_grad()\n",
    " class_loss = output[0, class_idx]\n",
    " class_loss.backward()\n",
    " \n",
    " # Calculate weights\n",
    " gradients = self.gradients[0]\n",
    " activations = self.activations[0]\n",
    " weights = torch.mean(gradients, dim=(1, 2), keepdim=True)\n",
    " \n",
    " # Generate CAM\n",
    " cam = torch.sum(weights * activations, dim=0)\n",
    " cam = F.relu(cam)\n",
    " cam = cam /(cam.max() + 1e-8) # Normalize\n",
    " \n",
    " return cam.cpu().numpy(), class_idx.item()\n",
    "\n",
    "# Get target layer based on model architecture\n",
    "if MODEL_NAME =='resnet50':\n",
    " target_layer = model.layer4[-1].conv3\n",
    "elif MODEL_NAME =='efficientnet_b0':\n",
    " target_layer = model.features[-1]\n",
    "elif MODEL_NAME =='mobilenet_v2':\n",
    " target_layer = model.features[-1]\n",
    "else:\n",
    " target_layer = list(model.children())[-2] # Fallback\n",
    "\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "print(\" Grad-CAM class initialized\")\n",
    "print(f\" Target layer: {target_layer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Grad-CAM for sample images\n",
    "def visualize_gradcam(image_path, model, gradcam, transform, label_classes, device):\n",
    "\"\"\"Generate and visualize Grad-CAM for an image\"\"\"\n",
    " # Load and preprocess image\n",
    " img = Image.open(image_path).convert('RGB')\n",
    " original_img = np.array(img)\n",
    " \n",
    " # Transform image\n",
    " input_tensor = transform(img).unsqueeze(0).to(device)\n",
    " \n",
    " # Get prediction\n",
    " with torch.no_grad():\n",
    " output = model(input_tensor)\n",
    " probs = F.softmax(output, dim=1)\n",
    " pred_class = output.argmax(dim=1).item()\n",
    " confidence = probs[0, pred_class].item()\n",
    " \n",
    " # Generate CAM\n",
    " cam, _ = gradcam.generate_cam(input_tensor, pred_class)\n",
    " \n",
    " # Resize CAM to original image size\n",
    " cam_resized = cv2.resize(cam,(original_img.shape[1], original_img.shape[0]))\n",
    " cam_resized = np.uint8(255 * cam_resized)\n",
    " heatmap = cv2.applyColorMap(cam_resized, cv2.COLORMAP_JET)\n",
    " heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    " \n",
    " # Overlay heatmap on original image\n",
    " overlayed = cv2.addWeighted(original_img, 0.6, heatmap, 0.4, 0)\n",
    " \n",
    " return original_img, overlayed, pred_class, confidence\n",
    "\n",
    "# Test transforms\n",
    "test_transform = transforms.Compose([\n",
    " transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    " transforms.ToTensor(),\n",
    " transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Select sample images for visualization\n",
    "num_samples = 6\n",
    "sample_indices = np.random.choice(len(df_test), num_samples, replace=False)\n",
    "sample_images = df_test.iloc[sample_indices]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GRAD-CAM VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n Generating Grad-CAM for {num_samples} sample images...\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(num_samples, 2, figsize=(14, 4*num_samples))\n",
    "fig.suptitle('Grad-CAM Visualizations', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "for idx,(_, row) in enumerate(sample_images.iterrows()):\n",
    " img_path = row['image_path']\n",
    " true_label = row['subcategory']\n",
    " \n",
    " try:\n",
    " original, overlayed, pred_class, confidence = visualize_gradcam(\n",
    " img_path, model, gradcam, test_transform, label_classes, device\n",
    " )\n",
    " \n",
    " pred_label = label_classes[pred_class]\n",
    " \n",
    " # Original image\n",
    " axes[idx, 0].imshow(original)\n",
    " axes[idx, 0].axis('off')\n",
    " axes[idx, 0].set_title(f'Original\\\\nTrue: {true_label}', fontsize=10, fontweight='bold')\n",
    " \n",
    " # Grad-CAM overlay\n",
    " axes[idx, 1].imshow(overlayed)\n",
    " axes[idx, 1].axis('off')\n",
    " color ='green' if pred_label == true_label else'red'\n",
    " axes[idx, 1].set_title(f'Grad-CAM\\\\nPred: {pred_label}({confidence*100:.1f}%)\\\\nTrue: {true_label}', \n",
    " fontsize=10, fontweight='bold', color=color)\n",
    " except Exception as e:\n",
    " axes[idx, 0].text(0.5, 0.5, f'Error: {str(e)}', ha='center', va='center')\n",
    " axes[idx, 0].axis('off')\n",
    " axes[idx, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Grad-CAM visualizations generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "### Key Findings\n",
    "- **Grad-CAM Visualizations**: Show which image regions the model focuses on\n",
    "- **Feature Importance**: Identify important features for classification\n",
    "- **Model Explanations**: Understand model decision-making process\n",
    "\n",
    "### Next Steps\n",
    "1. **SHAP Integration**: Install and use SHAP library for detailed explanations\n",
    "2. **Feature Importance**: Analyze layer-wise feature importance\n",
    "3. **Misclassification Analysis**: Deep dive into model errors with explanations\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Notes\n",
    "\n",
    "### Interpretability Techniques\n",
    "- **Grad-CAM**: Visualizes attention regions in images\n",
    "- **SHAP Values**: Provides feature-level explanations\n",
    "- **Feature Importance**: Identifies important model components\n",
    "\n",
    "### Usage\n",
    "- Use Grad-CAM to understand model focus areas\n",
    "- Analyze misclassifications with visual explanations\n",
    "- Validate model behavior on edge cases\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}